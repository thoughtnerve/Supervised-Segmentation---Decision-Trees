{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Data Analytic thinking helps you view business problems from a data perspective and understand the principles of extracting useful knowledge from data. A \"data perspective\" means a framework to systematically analyze problems through the use of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Science vs Data Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img style=\"align=middle\" src=\"ipyimages/datascience_vs_datamining-51.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Driven Decision Making"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img style=\"align=middle\" src=\"ipyimages/DDD-40.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Data Driven Decision making is the practice of basing decisions on the analysis of data, rather than purely on intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Data science is a set of fundamental principles that guide the extraction of knowledge from data, while Data mining is the extraction of knowledge from data, via technologies that implement these principles. So basically, data science is applied using data mining techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Predictive Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Predictive modeling is one of the main topics of data mining and can range from correlation to supervised segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supervised Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "(SS.10) The first concept within predictive modeling is supervised segmentation. Consider this example of a movie subscription service, like netflix, where there is a movie database that contains the history of whether a person watched a movie or not. Based on this history, we would like to predict which movie out of a potential list of unwatched movies, the person is likely to watch in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "(SS.20) So the target variable that we are interested in from the history is called *Watched*, which is a Yes or No value that represents whether the person watched a movie. Figuring out how we can find a formula, or in other words ***segment*** the history dataset to result in the value of *Watched = Yes*, is an example of supervised segmentation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "(SS.30) Additionally, in this case we are talking about predicting a Yes or No value for the target variable *Watched*, which means we are doing a ***classification***. If we were predicting a numeric value such as the likelihood of watching a movie, instead of a Yes or No value, then we would be doing a ***regression***. In other words, a regression is to predict a numeric value for a target variable of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "hide_input": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"742\"\n",
       "            height=\"700\"\n",
       "            src=\"https://docs.google.com/spreadsheets/d/1fJ0DKxFkMr2XjzyspZFLv6yOlHJvl639Qpap0O4U78E/edit#gid=0?align=center\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1064d5b00>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the IPython display module\n",
    "from IPython.display import IFrame\n",
    "# Use the IFrame display fuction to display a certain URL\n",
    "IFrame('https://docs.google.com/spreadsheets/d/1fJ0DKxFkMr2XjzyspZFLv6yOlHJvl639Qpap0O4U78E/edit#gid=0'\n",
    "       , width=742, height=700, align='center')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "(SS.50) So what is an informative attribute? An informative attribute is something that reduces uncertainity about the target variable. Let's for assume for a second that we perfectly understand the person's preferences. The person prefers the drama genre over others, but the length of the movie doesn't matter to the person. In this case the *Genre* attribute is more informative, as in it reduces the uncertainity of predicting whether the person will watch a movie or not. However the *Length* attribute is not informative because, in this case, it doesn't contribute to reducing the uncertainty of whether or not the person will watch a movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "hide_input": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"742\"\n",
       "            height=\"700\"\n",
       "            src=\"https://docs.google.com/spreadsheets/d/1fJ0DKxFkMr2XjzyspZFLv6yOlHJvl639Qpap0O4U78E/pubhtml?gid=0&fvid=1149565487&single=true&widget=false&headers=false\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1064d5320>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the IPython display module\n",
    "from IPython.display import IFrame\n",
    "# Use the IFrame display fuction to display a certain URL\n",
    "IFrame('https://docs.google.com/spreadsheets/d/1fJ0DKxFkMr2XjzyspZFLv6yOlHJvl639Qpap0O4U78E/pubhtml?gid=0&fvid=1149565487&single=true&widget=false&headers=false'\n",
    "       , width=742, height=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "(SS.40) The first part of supervised segmentation is to select the important informative attributes. The attributes we have in this dataset are *Title*,*Year*, *Genre*, *Length* and *Rating*. A single row of atrribute-values represent what is called a Feature vector. As an example, a feature vector here is: [ Title: The Godfather, Year : 1972, Genre: Crime/Drama, Length: 2h 55min, Rating: 9.2 ]. Each value in the feature vector is called a feature value. The attribute *Watched?* is the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Models, induction and deduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "(MID.10) We need a systematic mechanism or *Model* in order to figure out which attributes are informative. The process of creating models from data is called **Induction** and the procedure that creates a model from the data is called a *learner* or an *induction algorithm*. Induction essentially refers to generalizing from a specific case to general rules. We use the process of induction to create models from training data, which in this case is the movie dataset and then use *deduction* to use the modelMID/10 to predict target values for other instances of feature vectors, which in this case is the list of unwatched movies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<img style=\"align=middle\" src=\"ipyimages/movie_list_extended.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "(MID.20) Now let's try to create a model to figure out which attributes are more informative than others. Let's start with *Genre*. When we filter the history by *Genre = Drama* we get 2 watched movies and 2 unwatched movies. So there is a 50-50 chance of getting an expected result when we pick *Genre = Drama*, which is a highly random prediction. Let's try another attribute *Rating*. When we filter the history by *Rating > 8.8* we get 2 watched movies and 1 unwatched movie, which is a less random result. Let's try a combination of *Genre = Drama AND Year > 1995*. In this case we get 2 watched movies and no unwatched movies, which is the least random result of the attributes that we tested. So, by measuring the randomness we can determine how informative an attribute is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "(MID.30) The mathematical measure of uncertainity or randomness is called entropy. Entropy is defined as the following measure of probability : \n",
    "$$entropy = - p_1 log (p_1) - p_2 log (p_2) - ...$$\n",
    "Where $p_1$, in this example, is the probability of the target variable *Watched = Yes* and $p_2$ is the probability of the target variable *Watched = No* in the set of target values produced across the dataset. So in the case where we had a used attributes of *Genre = Drama AND Year > 1995*, the result was 2 watched movies and 0 unwatched movies. Which means p(Watched = Yes) = 1 hence and entropy = 1*log(1) = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "(MID.30) Without getting into the details of this equation, we can say that entropy can be numerically calculated. An entropy of 1 denotes the most randomness, while an entropy of 0 means least randomness. So this formula can help us determine how much an attribute decreases entropy of the segmentation it creates. This is also called *information gain* which measures the the change in entropy. Hence, we would want attributes that drive the highest information gain in prediction of a target value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Now lets take a look at the historical dataset. There are a total of 7 movies, with 3 of them watched and 4 unwatched movies. The probability of a watched movie is 3/7 = 42%. Lets partition this data into 2 segments based on the *Of Drama Genre Variable* into two segments. Segment 1 = *Of Drama Genre* = Y and Segment 2 = *Of Drama Genre* = N. Now Segment 1 has *Watched?* values of (N,Y, Y, N), which means a probability of Watched = 50% and Segment 2 has *Watched?* values of (N, Y, N) which means a probablity of Watched  = 33.3%. Comparing with the overall probability of of Watched - 42%, we have an important insight that movies of Genre = Drama have a higher probability of being watched. Now let us see if we can further segment this population by the *Released before 1995* variable. When we do this, we get (Y, Y). So we learn that a combination of "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Google](https://www.google.com)\n",
    "> ###Quote\n",
    "$x=y$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_input": true,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
