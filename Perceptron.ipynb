{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We discuss the problem of how to build a machine that can learn any given pattern of inputs and geenrate any desired pattern of outputs when it is poossible to do so. It is assumed that i.p patterns consists of 0s and 1s which indicate possible the presence and absence of different features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concept of a mp neuron was introduced earlier. This is an abstract math model of a brain cell. mp showd that such a brain cell model also known as a neuron model could operate like a logic gate or equivalently as an if-then logical rile. By conn. together a group of such comp. unit where each unit corresponding to a desrred if-then logical rule,. The responses of the entire collection of comp units could be combined with an additional computing unit to generate the response for the entire group. Thus given a sufficient number of these mp neurons or equivalently mp units, one can compute any arbitrary function where each mp unit esentially generates a respnse when certain conditions hold. That is, each mp unit functions as n if-then logical rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today the logic gate is the fundamental component common to all digitl computers and all smart machines. Althought he concept of being able to represent any collection of logical rules in an abstract neural network is very exciting, the ability to represent a collection of lofical rules, simply mans that there exists a way to wire up a collection of these computing units to realize any collection fof logical rules, Howevcer in practice we often do not know howmany rules we need and we don not know which rules we need to solve our computing problems. Ideally it would be very desirable if one could contruct away for the nn to learn frome xperience. When we simply train the nn with examples and the system would automatically learn how to connect itself together. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frank R;s goal was to develp a n.w of empty units which could learn from experience. Frank R caled this n/w a perceptron. Perceptrin contained 3 distinct groups of mp units. THe first group of mp units were called the sensor or input units. THe ops of the first gorup of mp units were connected to the second group of mp units, which were called association or hidden units. And finally the outputs of the hidden units were connected to the thrid groupof units which were called the response or o.p units. The connection from the i/p units to the hidden units were randomly fixed due to perhaps genetic disposition and not midifiable, while the connections from the hidden to the o.p units where assumed to be modifiabl e and they changed their values depending on the perceptrions experiences. The layerd peceptron acthicteutre should be able to learn arbitrary logical functions if only a small percentage of the hidden units generated responses for specific input patterns over the input units, to increase the chances of randomly obtaining some hidden units with approriate response pattterms, larger numbers of hidden units are locatend in random whcih hoped that these hidden units would act as feature detectors, which provided just the right features that it can solve the desired logic problem. So the connections from the i/p units to the hiddne units are fixed, so the interpretation of peripheral processing mecahnisms"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
