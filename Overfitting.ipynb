{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting is a problem that happens all the time in machine learning. Typically in ML analysis what you have is a dataset, and you are trying to learn the patterns in this dataset. You use those patterns to classify new examples. What overfitting is you train so specifically to the exact examples in the training dataset that the patterns you learn don't generalize well to new examples. And so what ends up happening is you actually do worse on trying to classify new examples than if you had tried to not come up with a set of rules thats so very specifically oriented for your training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This something that especially comes up when you have a relatively small dataset, or where you have lots of features relative to the number of training examples that you have. Also depending on the algorithm you are using and the exact way you are running it, the parameters that you have set, you can sometimes come up with decisions that can either be more intricate or they can be more general. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to avoid Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a number of ways you can control this but in general, something that you are always supposed to do is if you take your dataset and before you start training the algorithm, you take some subset of your data - maybe like 10-30% of the data and you set it aside. And you dont use that for your training and you call that your test set or your **holdout set** and the idea is that you have a set of data that is completely independent of your training data, and you can use that to evaluate how well your model is doing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What could go wrong with this approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having a **holdout set** is the industry standard, but something can go wrong with this approach as well. Lets say you get your dataset, and you take 25% of the data and put it in the holdout set (i.e. not to be used while training). You pick out a set of features that you are interested in and train the model and lets say you get 60% accuracy, which is a good start, but you could do better. So you are going to try to include a variable that you had excluded before and run your model again, and now you are at 63% accuracy and then you learn about some new paper suggested a transformation that you could do on some other features, so you apply that transformation and re-fit your model, and now you are at 65%, and you keep experimenting. Sometimes you see the improvements work and other times you see they suffer and you revert back to before. So you keep all the changes that improve the performance of the learner and discard those that dont work. So what you could actually end up doing is **overfitting to the holdout set**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By constantly checking back and forth on the holdout set, you could be indirectly overfitting against that and end up overfitting to both the training set and the holdout set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The \"reusable\" holdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"thresholdout\" algorithm idea is as follows. You have a dataset that you are training on and you have a holdout dataset that is accessed in a very specific way. You compute the accuracy of your model on your training data and also you go in and compute the accuracy of the holdout dataset, but you dont look at the results. What you do instead is you look at the **difference** between those two accuracy numbers. What this does is, if you are getting a little better accuracy on your training data you might be doing fine, but if you are getting a way better accuracy then you might be overfitting. The difference between the accuracy numbers is sort of reflective of how much you might be overfitting to your training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What thresholdout does is consider a threshold (e.g. 5%) and if the difference between the accuracies that have been computed is large, then the accuracy that you should get back from the algorithm is the accuracy that it computes on the test dataset. If the gap is big, then the accuracy number considered is that of the holdout set, but if the two numbers are closer than 5% then the algo is going to return the number of the training dataset. So there is always a little bit amout of ambiguity in what you are getting back, whether its the training data accuracy or the holdout set accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The thresholdout algorithm is obfuscating the accuracy so that you don't go hyperoptimize to one or the other. If you take the example of privacy, you might want to learn about the dataset as a whole but not about the individual members of the dataset, for example, trying to do ML on health information. You want aggregated statistics but don't want to learn about any specific individual. If we can learn about the dataset in aggregate but learning very little about an individual data element, then you can control the information leaked, and thus prevent overfitting."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
