{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false,
    "run_control": {
     "marked": true
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Predictive Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "hide_input": false,
    "run_control": {
     "marked": true
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Predictive modeling is one of the main topics of data mining and can range from correlation to supervised segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false,
    "run_control": {
     "marked": true
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supervised Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "hide_input": false,
    "hide_output": false,
    "run_control": {
     "marked": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Year</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Length</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Watched</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Raiders of the Lost Ark</td>\n",
       "      <td>80s</td>\n",
       "      <td>Action</td>\n",
       "      <td>Average</td>\n",
       "      <td>Average</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Lord of the Rings 3</td>\n",
       "      <td>00s</td>\n",
       "      <td>Drama</td>\n",
       "      <td>Long</td>\n",
       "      <td>High</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fight Club</td>\n",
       "      <td>90s</td>\n",
       "      <td>Drama</td>\n",
       "      <td>Average</td>\n",
       "      <td>High</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Braveheart</td>\n",
       "      <td>90s</td>\n",
       "      <td>Drama</td>\n",
       "      <td>Long</td>\n",
       "      <td>Average</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Inception</td>\n",
       "      <td>10s</td>\n",
       "      <td>Action</td>\n",
       "      <td>Average</td>\n",
       "      <td>High</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Godfather</td>\n",
       "      <td>70s</td>\n",
       "      <td>Drama</td>\n",
       "      <td>Average</td>\n",
       "      <td>High</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Aliens</td>\n",
       "      <td>80s</td>\n",
       "      <td>Horror</td>\n",
       "      <td>Average</td>\n",
       "      <td>Average</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Once upon a time in America</td>\n",
       "      <td>80s</td>\n",
       "      <td>Crime</td>\n",
       "      <td>Long</td>\n",
       "      <td>Average</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Title Year   Genre   Length   Rating Watched\n",
       "0      Raiders of the Lost Ark  80s  Action  Average  Average     Yes\n",
       "1      The Lord of the Rings 3  00s   Drama     Long     High     Yes\n",
       "2                   Fight Club  90s   Drama  Average     High     Yes\n",
       "3                   Braveheart  90s   Drama     Long  Average     Yes\n",
       "4                    Inception  10s  Action  Average     High      No\n",
       "5                The Godfather  70s   Drama  Average     High      No\n",
       "6                       Aliens  80s  Horror  Average  Average      No\n",
       "7  Once upon a time in America  80s   Crime     Long  Average      No"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Display movie history dataset\n",
    "import plotly\n",
    "import plotly.tools as tls\n",
    "from IPython.display import display\n",
    "from IPython.display import Image\n",
    "import cufflinks as cf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# Calculates the entropy of the given data set for the target attribute.\n",
    "def entropy(data, target_attr):\n",
    " \n",
    "    val_freq = {}\n",
    "    data_entropy = 0.0\n",
    " \n",
    "    # Calculate the frequency of each of the values in the target attr\n",
    "    for record in data:\n",
    "        \n",
    "        if (record[target_attr] in val_freq):\n",
    "            val_freq[record[target_attr]] += 1.0\n",
    "        else:\n",
    "            val_freq[record[target_attr]]  = 1.0\n",
    " \n",
    "    # Calculate the entropy of the data for the target attribute\n",
    "    for freq in val_freq.values():\n",
    "        data_entropy += (-freq/len(data)) * math.log(freq/len(data), 2) \n",
    " \n",
    "    return data_entropy\n",
    "\n",
    "def calc_entropy(criteria,target):    \n",
    "    print (\"Entropy (Randomness): \" ,  entropy(movie_history[criteria].to_dict('records'),target))    \n",
    "    return entropy(movie_history[criteria].to_dict('records'),target)\n",
    "\n",
    "py.sign_in('cloaked', 'wiwoxbjrvr')\n",
    "\n",
    "movie_history = pd.read_csv('https://goo.gl/xRfbvH')\n",
    "display(movie_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false,
    "run_control": {
     "marked": true
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "(SS.10) The first concept within predictive modeling is supervised segmentation. Consider this example of a movie subscription service, like netflix, where there is a movie database that contains the history of whether a person watched a movie or not. Based on this history, we would like to predict which movie out of a potential list of unwatched movies, the person is likely to watch in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false,
    "run_control": {
     "marked": true
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "(SS.20) So the target variable that we are interested in from the history is called *Watched*, which is a Yes or No value that represents whether the person watched a movie. Figuring out how we can find a formula, or in other words ***segment*** the history dataset to result in the value of *Watched = Yes*, is an example of supervised segmentation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false,
    "run_control": {
     "marked": true
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "(SS.30) Additionally, in this case we are talking about predicting a Yes or No value for the target variable *Watched*, which means we are doing a ***classification***. If we were predicting a numeric value such as the likelihood of watching a movie, instead of a Yes or No value, then we would be doing a ***regression***. In other words, a regression is to predict a numeric value for a target variable of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false,
    "run_control": {
     "marked": true
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "(SS.50) So what is an informative attribute? An informative attribute is something that reduces uncertainity about the target variable. Let's assume for a second that we perfectly understand the person's preferences. The person prefers the drama genre over others, but the length of the movie doesn't matter to the person. In this case the *Genre* attribute is more informative, as in it reduces the uncertainity of predicting whether the person will watch a movie or not. However the *Length* attribute is not informative because, in this case, it doesn't contribute to reducing the uncertainty of whether or not the person will watch a movie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "hide_input": false,
    "run_control": {
     "marked": true
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "(SS.40) The first part of supervised segmentation is to select the important informative attributes. The attributes we have in this dataset are *Title*,*Year*, *Genre*, *Length* and *Rating*. A single row of atrribute-values represent what is called a Feature vector. As an example, a feature vector here is: [ Title: The Godfather, Year : 70s, Genre: Drama, Length: Average, Rating: High ]. Each value in the feature vector is called a feature value. The attribute *Watched?* is the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false,
    "run_control": {
     "marked": true
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Models, induction and deduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false,
    "run_control": {
     "marked": true
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "(MID.10) We need a systematic mechanism or *Model* in order to figure out which attributes are informative. The process of creating models from data is called **Induction** and the procedure that creates a model from the data is called a *learner* or an *induction algorithm*. Induction essentially refers to generalizing from a specific case to general rules. We use the process of induction to create models from training data, which in this case is the movie dataset and then use *deduction* to use the model to predict target values for other instances of feature vectors, which in this case is the list of unwatched movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "hide_input": false,
    "run_control": {
     "marked": true
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy (Randomness):  1.0\n"
     ]
    }
   ],
   "source": [
    "#this is code\n",
    "no_filter = calc_entropy(criteria=movie_history.index >= 0,target='Watched')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false,
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "(MID.20) Now let's try to create a model to figure out which attributes are more informative than others. The first concept is to quantify the randomness of the dataset for the target variable. To quantify the randomness of the dataset we use a concept called entropy. Here the entropy of the dataset with respect to the target variable *Watched* is 1.0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false,
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "(MID.30) Entropy is a measure of probability or relative percentage of each unique value occuring among all values in a dataset . Entropy is defined by the following mathematical formula:\n",
    "$$entropy = - p_1 log (p_1) - p_2 log (p_2) - ...$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false,
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "(MID.40) Where $p_1$, in this example, is the probability of the target variable *Watched = Yes* and $p_2$ is the probability of the target variable *Watched = No* in the result set. Simply looking at this formula tells us that if all records had values of *Watched = Yes*, then $p_1 = 1 and p_2 = 0$ and therefore $log (p_1) = 0 $ which makes the entropy equal 0. A segment with an entropy of 0 denotes a **pure** segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "hide_input": false,
    "hide_output": false,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~cloaked/125.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Plot entropy concept\n",
    "data = [{'Watched': 'No'},\n",
    " {'Watched': 'No'},\n",
    " {'Watched': 'No'},\n",
    " {'Watched': 'No'},\n",
    " {'Watched': 'No'},\n",
    " {'Watched': 'No'},\n",
    " {'Watched': 'No'},\n",
    " {'Watched': 'No'}]\n",
    "\n",
    "\n",
    "def target_values(d):\n",
    "    vals = \"\"\n",
    "    for i in range(0,len(data)):\n",
    "        if i != 0:\n",
    "            vals = vals + \",\"\n",
    "        if i == len(data)/2:\n",
    "            vals = vals + \"<br>\"\n",
    "        vals = vals + d[i]['Watched']\n",
    "    return vals\n",
    "\n",
    "last_index = len(data) \n",
    "entropies = pd.DataFrame(columns=('X','Y','label'))\n",
    "   \n",
    "for i in range(0,last_index):        \n",
    "    entropies.loc[i]= [i,entropy(data,\"Watched\"),target_values(data)]\n",
    "    data[i]['Watched'] = 'Yes' \n",
    "entropies.loc[i+1]= [i+1,entropy(data,\"Watched\"),target_values(data)]\n",
    "\n",
    "trace1 = go.Scatter(\n",
    "    x=entropies.X,\n",
    "    y=entropies.Y,\n",
    "    mode='lines+markers+text',\n",
    "    name='Lines, Markers and Text',\n",
    "    text= entropies.label\n",
    ")\n",
    "\n",
    "display_data = [trace1]\n",
    "layout = go.Layout(\n",
    "    title = \n",
    "    'Entropy variation with the set of target values of <i>Watched</i> changing from all <b>No</b> to all <b>Yes</b> within the dataset',\n",
    "    showlegend=False, font=dict(family='Source Sans Pro')\n",
    ")\n",
    "fig = go.Figure(data=display_data, layout=layout)\n",
    "\n",
    "py.iplot(fig,filename='entropy_concept')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false,
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "(MID.50) When you take the movie dataset of 8 rows and plot the entropy assuming all movies are unwatched, which means an entropy of 0, to assuming all movies are watched, which also means an entropy of 0, we find that the entropy is the highest at the midpoint where 50% of the movies are watched and 50% of movies are unwatched, which is the point of maximum randomness. Hence entropy helps us measure the general disorder of the segmentation which is an indicator that could be used to quantify how much a particular attribute increases or decreases the disorder of the segmentation produced. To increase the predictability of the segmentation, we want to indentify attributes that decrease the randomness of the segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "hide_input": false,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy (Randomness):  0.8112781244591328\n"
     ]
    }
   ],
   "source": [
    "genre_entropy = calc_entropy(criteria=movie_history.Genre.str.contains('Drama'),target='Watched')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false,
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "(MID.60) In order to mathematically identify how much an attribute decreases randomness over the whole segmentation it creates, we use a concept called information gain that builds on the concept of entropy. \n",
    "To understand this better, lets filter this dataset by the attribute *Genre*. Looking at the movie history data, we see *Genre* has one of 4 values : Action, Drama, Horror, Crime. If we were to filter the history dataset by *Genre = Drama*, we would get 3 Watched movies and 1 unwatched movie and a resulting entropy of ~0.81. This entropy is less than the entropy of the whole dataset which was 1.0. Hence the information gain for a particular attrbute is derived as some function of change in entropy when the dataset is filtered or split on that attribute. When we plot the information gain for the attributes in the movie history dataset, we get the information gain values as 0.66, 0.34, 0.05, 0 for Year, Genre, Length and Rating respectively, showing that Year of the movie is the most informative attribute to predict whether a movie will be watched, followed by Genre and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "hide_input": false,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~cloaked/127.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Code to plot information gain\n",
    "# Calculates the information gain (reduction in entropy) \n",
    "#that would result by splitting the data on the chosen attribute (attr).\n",
    "def gain(data, attr, target_attr):\n",
    " \n",
    "    val_freq = {}\n",
    "    subset_entropy = 0.0\n",
    " \n",
    "    # Calculate the frequency of each of the values in the target attribute\n",
    "    for record in data:\n",
    "        if (record[attr] in val_freq):\n",
    "            val_freq[record[attr]] += 1.0\n",
    "        else:\n",
    "            val_freq[record[attr]]  = 1.0\n",
    " \n",
    "    # Calculate the sum of the entropy for each subset of records weighted by \n",
    "    #their probability of occuring in the training set.\n",
    "    for val in val_freq.keys():\n",
    "        val_prob = val_freq[val] / sum(val_freq.values())\n",
    "        data_subset = [record for record in data if record[attr] == val]\n",
    "        subset_entropy += val_prob * entropy(data_subset, target_attr)\n",
    " \n",
    "    # Subtract the entropy of the chosen attribute from the entropy of the whole\n",
    "    #data set with respect to the target attribute (and return it)\n",
    "    return round((entropy(data, target_attr) - subset_entropy),2)\n",
    " \n",
    "\n",
    "def calc_information_gain(criteria,attr,target):        \n",
    "    return gain(movie_history[criteria].to_dict('records'),attr, target)\n",
    "\n",
    "\n",
    "target_variable = 'Watched'\n",
    "gains = pd.DataFrame(columns=('Attribute','Gain'))\n",
    "for i in range (0,len(movie_history.columns)):\n",
    "    gains.loc[i]= [movie_history.columns.values[i],\n",
    "                   calc_information_gain(movie_history.index >= 0,\n",
    "                     movie_history.columns.values[i],target_variable)]\n",
    "\n",
    "#Exclude target variable\n",
    "gains = (gains[gains.Attribute.str.find(target_variable) == -1])  \n",
    "gains = (gains[gains.Attribute.str.find('Title') == -1])    \n",
    "trace1 = go.Bar(\n",
    "    x=gains.Attribute,\n",
    "    y=gains.Gain,\n",
    "    text=gains.Gain    \n",
    ")\n",
    "\n",
    "display_data = [trace1]\n",
    "layout = go.Layout(\n",
    "    title = \n",
    "    'Information gain by attribute',\n",
    "    showlegend=False, font=dict(family='Source Sans Pro')\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=display_data, layout=layout)\n",
    "\n",
    "py.iplot(fig,filename='information_gain_concept')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false,
    "run_control": {
     "marked": true
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "hide_input": false,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "#Code to build decision tree\n",
    "def choose_attribute(data_input, attributes, target_attr):\n",
    "    \"\"\" Cycles through all the attributes and returns the attribute with the\n",
    "    highest information gain \"\"\"\n",
    "\n",
    "    data = data_input[:]\n",
    "    best_gain = 0.0\n",
    "    best_attr = None\n",
    "\n",
    "    for attr in attributes:\n",
    "        if attr != target_attr:\n",
    "            gain1 = gain(data, attr, target_attr)\n",
    "            if gain1 > best_gain:\n",
    "                best_gain = gain1\n",
    "                best_attr = attr\n",
    "    \n",
    "    return best_attr\n",
    "\n",
    "def majority_value(data, target_attr):\n",
    "    \n",
    "    dic = {}\n",
    "    max_item = \"\"\n",
    "    for record in data:\n",
    "        dic[record[target_attr]] = dic.get(record[target_attr], 0) + 1\n",
    "    counts = [(j,i) for i,j in dic.items()]\n",
    "    count, max_item = max(counts)\n",
    "    del dic\n",
    "    return max_item\n",
    "\n",
    "def get_subset( data_input, best, val):\n",
    "    \"\"\" Returns a list of all the records in data with the value of attribute\n",
    "    matching the given value \"\"\"\n",
    "\n",
    "    data = data_input[:]\n",
    "    list = []\n",
    "\n",
    "    if not data:\n",
    "        return list\n",
    "    else:\n",
    "        for record in data:\n",
    "            if record[best] == val:\n",
    "                list.append(record)\n",
    "        return list\n",
    "\n",
    "\n",
    "def create_decision_tree(data_input, attributes, target_attr):\n",
    "    \"\"\" Returns a new decision tree \"\"\"\n",
    "    #print (\"--------------\")\n",
    "    #print (\"\\n\")\n",
    "    \n",
    "    #display (data_input)\n",
    "    #display (attributes)\n",
    "    \n",
    "    \n",
    "    data    = data_input[:]\n",
    "    vals    = [record[target_attr] for record in data]\n",
    "    default = majority_value(data, target_attr)\n",
    "\n",
    "    # If the dataset or attributes is empty, return the default value. Subtract\n",
    "    # 1 to account for target attributes\n",
    "    if not data or (len(attributes) - 1) <= 0:        \n",
    "        return default\n",
    "    # If all the records in dataset have the same values, return it\n",
    "    elif vals.count(vals[0]) == len(vals):\n",
    "        #print (\"same values\")\n",
    "        return vals[0]\n",
    "    else:\n",
    "        # Choose the next best attribute\n",
    "        best_attr = choose_attribute(data, attributes, target_attr)\n",
    "        # Create a new tree/node with the best attribute\n",
    "        tree = {best_attr:{}}\n",
    "        \n",
    "        #print (best_attr)\n",
    "\n",
    "        # Preprocess data, to generate a list containing the same data as data list\n",
    "        # but without duplicate\n",
    "\n",
    "        unique_data = []\n",
    "        for record in data:\n",
    "            if unique_data.count(record[best_attr]) <= 0:\n",
    "                unique_data.append(record[best_attr])\n",
    "\n",
    "        # Create a new decision tree for each of the values in the best\n",
    "        # attribute field\n",
    "        for val in unique_data:\n",
    "            # Create a subtree for the current value under the best field\n",
    "            subtree = create_decision_tree(\n",
    "                    get_subset(data, best_attr, val),\n",
    "                    [attr for attr in attributes if attr != best_attr],\n",
    "                    target_attr\n",
    "                    )\n",
    "\n",
    "            # Add the new subtree to the empty dictionary\n",
    "            tree[best_attr][val] = subtree\n",
    "\n",
    "    return tree\n",
    "\n",
    "\n",
    "attributes = movie_history.columns.tolist()\n",
    "attributes.remove('Watched')\n",
    "attributes.remove('Title')\n",
    "\n",
    "def print_tree(tree,string):\n",
    "    if type(tree) == dict:\n",
    "        print (list(tree.keys())[0],\"--->\")        \n",
    "        for item in list(tree.values())[0].keys():\n",
    "            #print ('***begin***')\n",
    "            print (string + \"\\t\\t\",item,':', end=\"\")                          \n",
    "            print_tree(list(tree.values())[0][item], string + \"\\t\")\n",
    "            #print ('***end***')\n",
    "    else:\n",
    "        #print ('else part')\n",
    "        print (tree)\n",
    "        \n",
    "def classify(tree, instance, default_class=None):\n",
    "    '''Returns a classification label for instance, given a decision tree'''\n",
    "    if not tree:  # if the node is empty, return the default class\n",
    "        return default_class\n",
    "    if not isinstance(tree, dict):  # if the node is a leaf, return its class label\n",
    "        return tree\n",
    "    attribute_index = list(tree.keys())[0]  # using list(dict.keys()) for Python 3 compatibility\n",
    "    attribute_values = list(tree.values())[0]\n",
    "    instance_attribute_value = instance[attribute_index]\n",
    "    if instance_attribute_value not in attribute_values:  # this value was not in training data\n",
    "        return default_class\n",
    "    # recursively traverse the subtree (branch) associated with instance_attribute_value\n",
    "    return classify(attribute_values[instance_attribute_value], instance, default_class)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "hide_input": false,
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "movie_history2 = pd.read_csv('https://goo.gl/ZdIl1w')\n",
    "tree=create_decision_tree(movie_history2.to_dict('records'),attributes,target_variable)        \n",
    "print_tree(tree, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    <script>\n",
      "    code_show=true; \n",
      "    function code_toggle(identifier) {\n",
      "    console.dir(identifier)\n",
      "    code_cell = $( \"div:contains(identifier):last\" ).parentsUntil('div.cell').last().parent();\n",
      "    console.dir(code_cell);\n",
      "     if (code_show){ \n",
      "         code_cell.show();\n",
      "     } else {\n",
      "         code_cell.show();\n",
      "     }\n",
      "     code_show = !code_show\n",
      "    } \n",
      "    \n",
      "    </script>\n",
      "     <form action=\"javascript:code_toggle('12345')\"> <input type=\"submit\" value= \"Click here to toggle on/off the raw code\"></form>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <script>\n",
       "    code_show=true; \n",
       "    function code_toggle(identifier) {\n",
       "    console.dir(identifier)\n",
       "    code_cell = $( \"div:contains(identifier):last\" ).parentsUntil('div.cell').last().parent();\n",
       "    console.dir(code_cell);\n",
       "     if (code_show){ \n",
       "         code_cell.show();\n",
       "     } else {\n",
       "         code_cell.show();\n",
       "     }\n",
       "     code_show = !code_show\n",
       "    } \n",
       "    \n",
       "    </script>\n",
       "     <form action=\"javascript:code_toggle('12345')\"> <input type=\"submit\" value= \"Click here to toggle on/off the raw code\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "def hide_code_message(message,identifier):\n",
    "    script = '''\n",
    "    <script>\n",
    "    code_show=true; \n",
    "    function code_toggle(identifier) {    \n",
    "    code_cell = $( \"div:contains(identifier):last\" ).parentsUntil('div.cell').last().parent();\n",
    "    console.dir(code_cell);\n",
    "     if (code_show){ \n",
    "         code_cell.show();\n",
    "     } else {\n",
    "         code_cell.show();\n",
    "     }\n",
    "     code_show = !code_show\n",
    "    } \n",
    "    \n",
    "    </script>\n",
    "    '''     \n",
    "    form_start1 = ''' <form action=\\\"javascript:code_toggle('''  \n",
    "    \n",
    "    identifier = \"\\'ID12345\\'\"\n",
    "    form_start2 = ''')\\\"> <input type=\\\"submit\\\" value= '''\n",
    "    #print (form_start1+identifier+form_start2)\n",
    "    form_end = '''></form>'''   \n",
    "    print (script + form_start1 + identifier + form_start2 + '\"' + message + '\"' + form_end)\n",
    "    return HTML(script + form_start1 + identifier + form_start2 + '\"' + message + '\"' + form_end)\n",
    "\n",
    "hide_code_message(\"Click here to toggle on/off the raw code\",'ID12345')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "hide_input": false,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classify' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-8a142e3b0719>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m  \u001b[0;34m'Year'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'00s'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m  'Watched': 'NA'}\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'classify' is not defined"
     ]
    }
   ],
   "source": [
    "#ID12345\n",
    "example = {'Genre': 'Horror',\n",
    " 'Length': 'Average',\n",
    " 'Rating': 'High',\n",
    " 'Title': 'Some title',\n",
    " 'Year': '00s',           \n",
    " 'Watched': 'NA'}\n",
    "classify(tree,example)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false,
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "(DT.10)Finding the most informative attribute alone is likely insufficient to produce segments that are predictive of target values. There is an elegant mechanism to combine multiple informative attributes to produce supervised segmentation for the target variable. This mechanism is called a decision tree. Decision trees are often used as predictive models. When presented with an example for which we don't know a classification, we can predict it's classification by traversing the decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false,
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "(DT.20) Using concepts we just learnt, let us build a decision tree model for our Movie subscription service to help us predict which movie out of a potential list of unwatched movies, the person is likely to watch in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false,
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "<img style=\"align=middle\" src=\"ipyimages/decision_tree.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false,
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "(DT.30) Creating a decision tree starts with choosing the most informative attribute in the dataset. This attribute becomes a **node** in the tree. For every value of this attribute, a child node is created. So, in our example, the most informative attribute is *Year*, and *Year* becomes our first node. *Year* has  possible values : 70s, 80s, 90s, 00s, 10s and a child node is created for each of these values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false,
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "(DT.40) When we flter the dataset with the value at a node, we either get a pure dataset, which means all target values are the same at this node, or we get an impure dataset. If the dataset is pure, then this node is a leaf node. If the dataset is impure then the tree is built again at this node for the impure dataset. The process continues till we reach all leaf nodes. When we string together the path of conditions from the top node to a leaf node, it gives  makes segments that points to a value of the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false,
    "run_control": {
     "marked": true
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Decision trees are an important concept due to their ability to learn to classify individual records in a dataset. They are used in many areas, from typical marketing scenarios such as targeting to airplane autopilots and medical diagnoses. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false,
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "A decision tree is essentially a series of if-then statements, which when applied to a record in a dataset, results in the classification of thar record. So in our example, the program that you create from a decision tree will be able to predict the likelihood of the person watching a movie from the history of movies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false,
    "run_control": {
     "marked": true
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false,
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "Christopher Roach: Building Decision Trees in Python http://www.onlamp.com/pub/a/python/2006/02/09/ai_decision_trees.html\n",
    "\n",
    "Joe McCarthy: Python for Data Science\n",
    "https://github.com/gumption/Python_for_Data_Science/blob/\n",
    "master/4_Python_Simple_Decision_Tree.ipynb\n",
    "\n",
    "Provost Foster, Tom Fawcett: Data Science for Business\n",
    "https://www.safaribooksonline.com/library/view/data-science-for/9781449374273/"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
