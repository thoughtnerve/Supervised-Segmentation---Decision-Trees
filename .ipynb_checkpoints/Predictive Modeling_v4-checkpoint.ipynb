{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Predictive Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Predictive modeling is one of the main topics of data mining and can range from correlation to supervised segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supervised Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "hide_input": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Year</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Length</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Watched</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Raiders of the Lost Ark</td>\n",
       "      <td>80s</td>\n",
       "      <td>Action</td>\n",
       "      <td>Average</td>\n",
       "      <td>Average</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Lord of the Rings 3</td>\n",
       "      <td>00s</td>\n",
       "      <td>Drama</td>\n",
       "      <td>Long</td>\n",
       "      <td>High</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fight Club</td>\n",
       "      <td>90s</td>\n",
       "      <td>Drama</td>\n",
       "      <td>Average</td>\n",
       "      <td>High</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Braveheart</td>\n",
       "      <td>90s</td>\n",
       "      <td>Drama</td>\n",
       "      <td>Long</td>\n",
       "      <td>Average</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Inception</td>\n",
       "      <td>10s</td>\n",
       "      <td>Action</td>\n",
       "      <td>Average</td>\n",
       "      <td>High</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Godfather</td>\n",
       "      <td>70s</td>\n",
       "      <td>Drama</td>\n",
       "      <td>Average</td>\n",
       "      <td>High</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Aliens</td>\n",
       "      <td>80s</td>\n",
       "      <td>Horror</td>\n",
       "      <td>Average</td>\n",
       "      <td>Average</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Once upon a time in America</td>\n",
       "      <td>80s</td>\n",
       "      <td>Crime</td>\n",
       "      <td>Long</td>\n",
       "      <td>Average</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Title Year   Genre   Length   Rating Watched\n",
       "0      Raiders of the Lost Ark  80s  Action  Average  Average     Yes\n",
       "1      The Lord of the Rings 3  00s   Drama     Long     High     Yes\n",
       "2                   Fight Club  90s   Drama  Average     High     Yes\n",
       "3                   Braveheart  90s   Drama     Long  Average     Yes\n",
       "4                    Inception  10s  Action  Average     High      No\n",
       "5                The Godfather  70s   Drama  Average     High      No\n",
       "6                       Aliens  80s  Horror  Average  Average      No\n",
       "7  Once upon a time in America  80s   Crime     Long  Average      No"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import python_libraries\n",
    "import plotly\n",
    "import plotly.tools as tls\n",
    "from IPython.display import display\n",
    "import cufflinks as cf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# Calculates the entropy of the given data set for the target attribute.\n",
    "def entropy(data, target_attr):\n",
    " \n",
    "    val_freq = {}\n",
    "    data_entropy = 0.0\n",
    " \n",
    "    # Calculate the frequency of each of the values in the target attr\n",
    "    for record in data:\n",
    "        \n",
    "        if (record[target_attr] in val_freq):\n",
    "            val_freq[record[target_attr]] += 1.0\n",
    "        else:\n",
    "            val_freq[record[target_attr]]  = 1.0\n",
    " \n",
    "    # Calculate the entropy of the data for the target attribute\n",
    "    for freq in val_freq.values():\n",
    "        data_entropy += (-freq/len(data)) * math.log(freq/len(data), 2) \n",
    " \n",
    "    return data_entropy\n",
    "\n",
    "\n",
    "def calc_entropy(criteria,target):    \n",
    "    print (\"Entropy (Randomness): \" +  str(entropy(movie_history[criteria].to_dict('records'),target)))\n",
    "    display (movie_history[criteria])\n",
    "    return entropy(movie_history[criteria].to_dict('records'),target)\n",
    "\n",
    "# Calculates the information gain (reduction in entropy) \n",
    "#that would result by splitting the data on the chosen attribute (attr).\n",
    "def gain(data, attr, target_attr):\n",
    " \n",
    "    val_freq = {}\n",
    "    subset_entropy = 0.0\n",
    " \n",
    "    # Calculate the frequency of each of the values in the target attribute\n",
    "    for record in data:\n",
    "        if (record[attr] in val_freq):\n",
    "            val_freq[record[attr]] += 1.0\n",
    "        else:\n",
    "            val_freq[record[attr]]  = 1.0\n",
    " \n",
    "    # Calculate the sum of the entropy for each subset of records weighted by \n",
    "    #their probability of occuring in the training set.\n",
    "    for val in val_freq.keys():\n",
    "        val_prob = val_freq[val] / sum(val_freq.values())\n",
    "        data_subset = [record for record in data if record[attr] == val]\n",
    "        subset_entropy += val_prob * entropy(data_subset, target_attr)       \n",
    " \n",
    "    # Subtract the entropy of the chosen attribute from the entropy of the whole\n",
    "    #data set with respect to the target attribute (and return it)\n",
    "    return round((entropy(data, target_attr) - subset_entropy),2)\n",
    " \n",
    "def calc_information_gain_entropy(data,attr,attr_name,target):    \n",
    "    print (\"Information gain with attribute \" + attr_name + \" is : \" + str(gain(movie_history[criteria].to_dict('records'),attr, target)))    \n",
    "    return gain(data.to_dict('records'),attr, target)\n",
    "\n",
    "py.sign_in('cloaked', 'wiwoxbjrvr')\n",
    "\n",
    "movie_history = pd.read_csv('https://docs.google.com/spreadsheets/d/1fJ0DKxFkMr2XjzyspZFLv6yOlHJvl639Qpap0O4U78E/pub?gid=0&single=true&output=csv')\n",
    "display(movie_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "(SS.10) The first concept within predictive modeling is supervised segmentation. Consider this example of a movie subscription service, like netflix, where there is a movie database that contains the history of whether a person watched a movie or not. Based on this history, we would like to predict which movie out of a potential list of unwatched movies, the person is likely to watch in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "(SS.20) So the target variable that we are interested in from the history is called *Watched*, which is a Yes or No value that represents whether the person watched a movie. Figuring out how we can find a formula, or in other words ***segment*** the history dataset to result in the value of *Watched = Yes*, is an example of supervised segmentation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "(SS.30) Additionally, in this case we are talking about predicting a Yes or No value for the target variable *Watched*, which means we are doing a ***classification***. If we were predicting a numeric value such as the likelihood of watching a movie, instead of a Yes or No value, then we would be doing a ***regression***. In other words, a regression is to predict a numeric value for a target variable of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "(SS.50) So what is an informative attribute? An informative attribute is something that reduces uncertainity about the target variable. Let's assume for a second that we perfectly understand the person's preferences. The person prefers the drama genre over others, but the length of the movie doesn't matter to the person. In this case the *Genre* attribute is more informative, as in it reduces the uncertainity of predicting whether the person will watch a movie or not. However the *Length* attribute is not informative because, in this case, it doesn't contribute to reducing the uncertainty of whether or not the person will watch a movie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "(SS.40) The first part of supervised segmentation is to select the important informative attributes. The attributes we have in this dataset are *Title*,*Year*, *Genre*, *Length* and *Rating*. A single row of atrribute-values represent what is called a Feature vector. As an example, a feature vector here is: [ Title: The Godfather, Year : 70s, Genre: Drama, Length: Average, Rating: High ]. Each value in the feature vector is called a feature value. The attribute *Watched?* is the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Models, induction and deduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "(MID.10) We need a systematic mechanism or *Model* in order to figure out which attributes are informative. The process of creating models from data is called **Induction** and the procedure that creates a model from the data is called a *learner* or an *induction algorithm*. Induction essentially refers to generalizing from a specific case to general rules. We use the process of induction to create models from training data, which in this case is the movie dataset and then use *deduction* to use the model to predict target values for other instances of feature vectors, which in this case is the list of unwatched movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy (Randomness): 1.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Year</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Length</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Watched</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Raiders of the Lost Ark</td>\n",
       "      <td>80s</td>\n",
       "      <td>Action</td>\n",
       "      <td>Average</td>\n",
       "      <td>Average</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Lord of the Rings 3</td>\n",
       "      <td>00s</td>\n",
       "      <td>Drama</td>\n",
       "      <td>Long</td>\n",
       "      <td>High</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fight Club</td>\n",
       "      <td>90s</td>\n",
       "      <td>Drama</td>\n",
       "      <td>Average</td>\n",
       "      <td>High</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Braveheart</td>\n",
       "      <td>90s</td>\n",
       "      <td>Drama</td>\n",
       "      <td>Long</td>\n",
       "      <td>Average</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Inception</td>\n",
       "      <td>10s</td>\n",
       "      <td>Action</td>\n",
       "      <td>Average</td>\n",
       "      <td>High</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Godfather</td>\n",
       "      <td>70s</td>\n",
       "      <td>Drama</td>\n",
       "      <td>Average</td>\n",
       "      <td>High</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Aliens</td>\n",
       "      <td>80s</td>\n",
       "      <td>Horror</td>\n",
       "      <td>Average</td>\n",
       "      <td>Average</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Once upon a time in America</td>\n",
       "      <td>80s</td>\n",
       "      <td>Crime</td>\n",
       "      <td>Long</td>\n",
       "      <td>Average</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Title Year   Genre   Length   Rating Watched\n",
       "0      Raiders of the Lost Ark  80s  Action  Average  Average     Yes\n",
       "1      The Lord of the Rings 3  00s   Drama     Long     High     Yes\n",
       "2                   Fight Club  90s   Drama  Average     High     Yes\n",
       "3                   Braveheart  90s   Drama     Long  Average     Yes\n",
       "4                    Inception  10s  Action  Average     High      No\n",
       "5                The Godfather  70s   Drama  Average     High      No\n",
       "6                       Aliens  80s  Horror  Average  Average      No\n",
       "7  Once upon a time in America  80s   Crime     Long  Average      No"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "no_filter = calc_entropy(criteria=movie_history.index >= 0,target='Watched')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(MID.20) Now let's try to create a model to figure out which attributes are more informative than others. The first concept is to quantify the randomness of the dataset for the target variable. To quantify the randomness of the dataset we use a concept called entropy. Here the entropy of the dataset with respect to the target variable *Watched* is 1.0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(MID.30) Entropy is a measure of probability or relative percentage of each unique value occuring among all values in a dataset . Entropy is defined by the following mathematical formula:\n",
    "$$entropy = - p_1 log (p_1) - p_2 log (p_2) - ...$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(MID.40) Where $p_1$, in this example, is the probability of the target variable *Watched = Yes* and $p_2$ is the probability of the target variable *Watched = No* in the result set. Simply looking at this formula tells us that if all records had values of *Watched = Yes*, then $p_1 = 1 and p_2 = 0$ and therefore $log (p_1) = 0 $ which makes the entropy equal 0. A segment with an entropy of 0 denotes a **pure** segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~cloaked/125.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [{'Watched': 'No'},\n",
    " {'Watched': 'No'},\n",
    " {'Watched': 'No'},\n",
    " {'Watched': 'No'},\n",
    " {'Watched': 'No'},\n",
    " {'Watched': 'No'},\n",
    " {'Watched': 'No'},\n",
    " {'Watched': 'No'}]\n",
    "\n",
    "\n",
    "def target_values(d):\n",
    "    vals = \"\"\n",
    "    for i in range(0,len(data)):\n",
    "        if i != 0:\n",
    "            vals = vals + \",\"\n",
    "        if i == len(data)/2:\n",
    "            vals = vals + \"<br>\"\n",
    "        vals = vals + d[i]['Watched']\n",
    "    return vals\n",
    "\n",
    "last_index = len(data) \n",
    "entropies = pd.DataFrame(columns=('X','Y','label'))\n",
    "   \n",
    "for i in range(0,last_index):        \n",
    "    entropies.loc[i]= [i,entropy(data,\"Watched\"),target_values(data)]\n",
    "    data[i]['Watched'] = 'Yes' \n",
    "entropies.loc[i+1]= [i+1,entropy(data,\"Watched\"),target_values(data)]\n",
    "\n",
    "trace1 = go.Scatter(\n",
    "    x=entropies.X,\n",
    "    y=entropies.Y,\n",
    "    mode='lines+markers+text',\n",
    "    name='Lines, Markers and Text',\n",
    "    text= entropies.label\n",
    ")\n",
    "\n",
    "display_data = [trace1]\n",
    "layout = go.Layout(\n",
    "    title = \n",
    "    'Entropy variation with the set of target values of <i>Watched</i> changing from all <b>No</b> to all <b>Yes</b> within the dataset',\n",
    "    showlegend=False, font=dict(family='Source Sans Pro')\n",
    ")\n",
    "fig = go.Figure(data=display_data, layout=layout)\n",
    "\n",
    "py.iplot(fig,filename='entropy_concept')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(MID.50) When you take the movie dataset of 8 rows and plot the entropy assuming all movies are unwatched, which means an entropy of 0, to assuming all movies are watched, which also means an entropy of 0, we find that the entropy is the highest at the midpoint where 50% of the movies are watched and 50% of movies are unwatched, which is the point of maximum randomness. Hence entropy helps us measure the general disorder of the segmentation which is an indicator that could be used to quantify how much a particular attribute increases or decreases the disorder of the segmentation produced. To increase the predictability of the segmentation, we want to indentify attributes that decrease the randomness of the segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(MID.60) In order to mathematically identify how much an attribute decreases randomness over the whole segmentation it creates, we use a concept called information gain. Here is a plot for information gain of each of the attributes in the movie history dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculates the information gain (reduction in entropy) \n",
    "#that would result by splitting the data on the chosen attribute (attr).\n",
    "def gain(data, attr, target_attr):\n",
    " \n",
    "    val_freq = {}\n",
    "    subset_entropy = 0.0\n",
    " \n",
    "    # Calculate the frequency of each of the values in the target attribute\n",
    "    for record in data:\n",
    "        if (record[attr] in val_freq):\n",
    "            val_freq[record[attr]] += 1.0\n",
    "        else:\n",
    "            val_freq[record[attr]]  = 1.0\n",
    " \n",
    "    # Calculate the sum of the entropy for each subset of records weighted by \n",
    "    #their probability of occuring in the training set.\n",
    "    for val in val_freq.keys():\n",
    "        val_prob = val_freq[val] / sum(val_freq.values())\n",
    "        data_subset = [record for record in data if record[attr] == val]\n",
    "        subset_entropy += val_prob * entropy(data_subset, target_attr)\n",
    " \n",
    "    # Subtract the entropy of the chosen attribute from the entropy of the whole\n",
    "    #data set with respect to the target attribute (and return it)\n",
    "    return round((entropy(data, target_attr) - subset_entropy),2)\n",
    " \n",
    "\n",
    "def calc_information_gain(criteria,attr,target):        \n",
    "    return gain(movie_history[criteria].to_dict('records'),attr, target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~cloaked/127.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_variable = 'Watched'\n",
    "gains = pd.DataFrame(columns=('Attribute','Gain'))\n",
    "for i in range (0,len(movie_history.columns)):\n",
    "    gains.loc[i]= [movie_history.columns.values[i],\n",
    "                   calc_information_gain(movie_history.index >= 0,\n",
    "                     movie_history.columns.values[i],target_variable)]\n",
    "\n",
    "#Exclude target variable\n",
    "gains = (gains[gains.Attribute.str.find(target_variable) == -1])  \n",
    "gains = (gains[gains.Attribute.str.find('Title') == -1])    \n",
    "trace1 = go.Bar(\n",
    "    x=gains.Attribute,\n",
    "    y=gains.Gain,\n",
    "    text=gains.Gain    \n",
    ")\n",
    "\n",
    "display_data = [trace1]\n",
    "layout = go.Layout(\n",
    "    title = \n",
    "    'Information gain by attribute',\n",
    "    showlegend=False, font=dict(family='Source Sans Pro')\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=display_data, layout=layout)\n",
    "\n",
    "py.iplot(fig,filename='information_gain_concept')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "code_folding": [
     6
    ],
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title 0.99\n",
      "Year 0.99\n",
      "Genre 0.52\n",
      "Length 0.13\n",
      "Rating 0.02\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~cloaked/125.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_variable = 'Watched'\n",
    "gains = pd.DataFrame(columns=('Attribute','Gain'))\n",
    "for value in movie_history.columns.values:\n",
    "    if value != target_variable:\n",
    "        print (value + \" \" + str((calc_information_gain(movie_history.index > 0,value,target_variable))))\n",
    "        \n",
    "data = [{'Watched': 'No'},\n",
    " {'Watched': 'No'},\n",
    " {'Watched': 'No'},\n",
    " {'Watched': 'No'},\n",
    " {'Watched': 'No'},\n",
    " {'Watched': 'No'},\n",
    " {'Watched': 'No'},\n",
    " {'Watched': 'No'}]\n",
    "\n",
    "\n",
    "def target_values(d):\n",
    "    vals = \"\"\n",
    "    for i in range(0,len(data)):\n",
    "        if i != 0:\n",
    "            vals = vals + \",\"\n",
    "        if i == len(data)/2:\n",
    "            vals = vals + \"<br>\"\n",
    "        vals = vals + d[i]['Watched']\n",
    "    return vals\n",
    "\n",
    "last_index = len(data) \n",
    "\n",
    "   \n",
    "for i in range(0,last_index):        \n",
    "    entropies.loc[i]= [i,entropy(data,\"Watched\"),target_values(data)]\n",
    "    data[i]['Watched'] = 'Yes' \n",
    "entropies.loc[i+1]= [i+1,entropy(data,\"Watched\"),target_values(data)]\n",
    "\n",
    "trace1 = go.Scatter(\n",
    "    x=entropies.X,\n",
    "    y=entropies.Y,\n",
    "    mode='lines+markers+text',\n",
    "    name='Lines, Markers and Text',\n",
    "    text= entropies.label\n",
    ")\n",
    "\n",
    "display_data = [trace1]\n",
    "layout = go.Layout(\n",
    "    title = \n",
    "    'Entropy variation with the set of target values of <i>Watched</i> changing from all <b>No</b> to all <b>Yes</b> within the dataset',\n",
    "    showlegend=False, font=dict(family='Source Sans Pro')\n",
    ")\n",
    "fig = go.Figure(data=display_data, layout=layout)\n",
    "\n",
    "py.iplot(fig,filename='entropy_concept')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~cloaked/139.embed\" height=\"700px\" width=\"700px\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace1 = Scatter(\n",
    "    x=[2.6644980675659595, 1.2509801194371117, None, 2.6644980675659595, 4.086390232868387, None],\n",
    "    y=[-4.6076777515108125, -4.559748930034137, None, -4.6076777515108125, -4.645195358685002, None],\n",
    "    hoverinfo='none',\n",
    "    line=Line(\n",
    "        color='rgb(210,210,210)',\n",
    "        width=2\n",
    "    ),\n",
    "    mode='lines',\n",
    "    name='Trace 0, y'\n",
    ")\n",
    "trace2 = Scatter(\n",
    "    x=[2.6644980675659595, 1.2509801194371117, 4.086390232868387, 0.7158689493718534, 0.4215468767348411, 4.910287453614753],\n",
    "    y=[-4.6076777515108125, -4.559748930034137, -4.645195358685002, -3.3266007351115277, -5.703800295665652, -5.675463958802986],\n",
    "    hoverinfo='text',\n",
    "    marker=Marker(\n",
    "        color='#6175c1',\n",
    "        line=Line(\n",
    "            color='rgb(50,50,50)',\n",
    "            width=2\n",
    "        ),\n",
    "        size=20,\n",
    "        symbol='dot'\n",
    "    ),\n",
    "    mode='markers',\n",
    "    name='',\n",
    "    opacity=0.8,\n",
    "    text=['0', '1', '2', '3', '4', '5']\n",
    ")\n",
    "data = Data([trace1, trace2])\n",
    "layout = Layout(\n",
    "    annotations=Annotations([\n",
    "        Annotation(\n",
    "            x=2.6644980675659595,\n",
    "            y=-4.6076777515108125,\n",
    "            font=Font(\n",
    "                color='rgb(250,250,250)',\n",
    "                size=10\n",
    "            ),\n",
    "            showarrow=False,\n",
    "            text='0',\n",
    "            xref='x1',\n",
    "            yref='y1'\n",
    "        ),\n",
    "        Annotation(\n",
    "            x=1.2509801194371117,\n",
    "            y=-4.559748930034137,\n",
    "            font=Font(\n",
    "                color='rgb(250,250,250)',\n",
    "                size=10\n",
    "            ),\n",
    "            showarrow=False,\n",
    "            text='1',\n",
    "            xref='x1',\n",
    "            yref='y1'\n",
    "        ),\n",
    "        Annotation(\n",
    "            x=4.086390232868387,\n",
    "            y=-4.645195358685002,\n",
    "            font=Font(\n",
    "                color='rgb(250,250,250)',\n",
    "                size=10\n",
    "            ),\n",
    "            showarrow=False,\n",
    "            text='2',\n",
    "            xref='x1',\n",
    "            yref='y1'\n",
    "        ),\n",
    "        Annotation(\n",
    "            x=0.7158689493718534,\n",
    "            y=-3.3266007351115277,\n",
    "            font=Font(\n",
    "                color='rgb(250,250,250)',\n",
    "                size=10\n",
    "            ),\n",
    "            showarrow=False,\n",
    "            text='3',\n",
    "            xref='x1',\n",
    "            yref='y1'\n",
    "        ),\n",
    "        Annotation(\n",
    "            x=0.4215468767348411,\n",
    "            y=-5.703800295665652,\n",
    "            font=Font(\n",
    "                color='rgb(250,250,250)',\n",
    "                size=10\n",
    "            ),\n",
    "            showarrow=False,\n",
    "            text='4',\n",
    "            xref='x1',\n",
    "            yref='y1'\n",
    "        ),\n",
    "        Annotation(\n",
    "            x=4.910287453614753,\n",
    "            y=-5.675463958802986,\n",
    "            font=Font(\n",
    "                color='rgb(250,250,250)',\n",
    "                size=10\n",
    "            ),\n",
    "            showarrow=False,\n",
    "            text='5',\n",
    "            xref='x1',\n",
    "            yref='y1'\n",
    "        )\n",
    "    ]),\n",
    "    autosize=False,\n",
    "    font=Font(\n",
    "        size=14\n",
    "    ),\n",
    "    height=700,\n",
    "    hovermode='closest',\n",
    "    margin=Margin(\n",
    "        r=40,\n",
    "        t=100,\n",
    "        b=85,\n",
    "        l=40\n",
    "    ),\n",
    "    plot_bgcolor='#EFECEA',\n",
    "    showlegend=False,\n",
    "    title='Tree',\n",
    "    width=700,\n",
    "    xaxis=XAxis(\n",
    "        showgrid=False,\n",
    "        showline=False,\n",
    "        showticklabels=False,\n",
    "        title='',\n",
    "        zeroline=False\n",
    "    ),\n",
    "    yaxis=YAxis(\n",
    "        showgrid=False,\n",
    "        showline=False,\n",
    "        showticklabels=False,\n",
    "        title='',\n",
    "        zeroline=False\n",
    "    )\n",
    ")\n",
    "fig = Figure(data=data, layout=layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Google](https://www.google.com)\n",
    "> ###Quote\n",
    "$x=y$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Decision trees are an important concept due to their ability to learn to classify individual records in a dataset. They are used in many areas, from typical marketing scenarios such as targeting to airplane autopilots and medical diagnoses. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decision tree is essentially a series of if-then statements, which when applied to a record in a dataset, results in the classification of thar record. So in our example, the program that you create from a decision tree will be able to predict the likelihood of the person watching a movie from the history of movies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At every step of the decision tree, the next most informative attribute is selected to divide the dataset according to some predefined criteria. One of the most popular approaches uses the concept of entropy to calculate which attrubute is best to use for dividing the data into subgroups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Christopher Roach: Building Decision Trees in Python http://www.onlamp.com/pub/a/python/2006/02/09/ai_decision_trees.html"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
