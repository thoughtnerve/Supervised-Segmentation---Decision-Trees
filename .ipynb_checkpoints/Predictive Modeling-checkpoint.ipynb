{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Predictive Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Predictive modeling is one of the main topics of data mining and can range from correlation to supervised segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supervised Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "(SS.10) The first concept within predictive modeling is supervised segmentation. Consider this example of a movie subscription service, like netflix, where there is a movie database that contains the history of whether a person watched a movie or not. Based on this history, we would like to predict which movie out of a potential list of unwatched movies, the person is likely to watch in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "(SS.20) So the target variable that we are interested in from the history is called *Watched*, which is a Yes or No value that represents whether the person watched a movie. Figuring out how we can find a formula, or in other words ***segment*** the history dataset to result in the value of *Watched = Yes*, is an example of supervised segmentation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "(SS.30) Additionally, in this case we are talking about predicting a Yes or No value for the target variable *Watched*, which means we are doing a ***classification***. If we were predicting a numeric value such as the likelihood of watching a movie, instead of a Yes or No value, then we would be doing a ***regression***. In other words, a regression is to predict a numeric value for a target variable of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "hide_input": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"742\"\n",
       "            height=\"700\"\n",
       "            src=\"https://docs.google.com/spreadsheets/d/1fJ0DKxFkMr2XjzyspZFLv6yOlHJvl639Qpap0O4U78E/edit#gid=0?align=center\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1064d5b00>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the IPython display module\n",
    "from IPython.display import IFrame\n",
    "# Use the IFrame display fuction to display a certain URL\n",
    "IFrame('https://docs.google.com/spreadsheets/d/1fJ0DKxFkMr2XjzyspZFLv6yOlHJvl639Qpap0O4U78E/edit#gid=0'\n",
    "       , width=742, height=700, align='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "hide_input": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"742\"\n",
       "            height=\"700\"\n",
       "            src=\"https://docs.google.com/spreadsheets/d/1fJ0DKxFkMr2XjzyspZFLv6yOlHJvl639Qpap0O4U78E/pubhtml?gid=0&fvid=1149565487&single=true&widget=false&headers=false\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1064d5320>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the IPython display module\n",
    "from IPython.display import IFrame\n",
    "# Use the IFrame display fuction to display a certain URL\n",
    "IFrame('https://docs.google.com/spreadsheets/d/1fJ0DKxFkMr2XjzyspZFLv6yOlHJvl639Qpap0O4U78E/pubhtml?gid=0&fvid=1149565487&single=true&widget=false&headers=false'\n",
    "       , width=742, height=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "(SS.50) So what is an informative attribute? An informative attribute is something that reduces uncertainity about the target variable. Let's for assume for a second that we perfectly understand the person's preferences. The person prefers the drama genre over others, but the length of the movie doesn't matter to the person. In this case the *Genre* attribute is more informative, as in it reduces the uncertainity of predicting whether the person will watch a movie or not. However the *Length* attribute is not informative because, in this case, it doesn't contribute to reducing the uncertainty of whether or not the person will watch a movie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "(SS.40) The first part of supervised segmentation is to select the important informative attributes. The attributes we have in this dataset are *Title*,*Year*, *Genre*, *Length* and *Rating*. A single row of atrribute-values represent what is called a Feature vector. As an example, a feature vector here is: [ Title: The Godfather, Year : 1972, Genre: Crime/Drama, Length: 2h 55min, Rating: 9.2 ]. Each value in the feature vector is called a feature value. The attribute *Watched?* is the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## Models, induction and deduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "(MID.10) We need a systematic mechanism or *Model* in order to figure out which attributes are informative. The process of creating models from data is called **Induction** and the procedure that creates a model from the data is called a *learner* or an *induction algorithm*. Induction essentially refers to generalizing from a specific case to general rules. We use the process of induction to create models from training data, which in this case is the movie dataset and then use *deduction* to use the modelMID/10 to predict target values for other instances of feature vectors, which in this case is the list of unwatched movies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "(MID.20) Now let's try to create a model to figure out which attributes are more informative than others. Let's start with *Genre*. When we filter the history by *Genre = Drama* we get 2 watched movies and 2 unwatched movies. So there is a 50-50 chance of getting an expected result when we pick *Genre = Drama*, which is a highly random prediction. Let's try another attribute *Rating*. When we filter the history by *Rating > 8.8* we get 2 watched movies and 1 unwatched movie, which is a less random result. Let's try a combination of *Genre = Drama AND Year > 1995*. In this case we get 2 watched movies and no unwatched movies, which is the least random result of the attributes that we tested. So, by measuring the randomness we can determine how informative an attribute is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$entropy = - p_1 log (p_1) - p_2 log (p_2) - ...$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "(MID.30) The mathematical measure of uncertainity or randomness is called entropy. Entropy is defined as the following measure of probability : \n",
    "$$entropy = - p_1 log (p_1) - p_2 log (p_2) - ...$$\n",
    "Where $p_1$, in this example, is the probability of the target variable *Watched = Yes* and $p_2$ is the probability of the target variable *Watched = No* in the set of target values produced across the dataset. So in this case where we had used attributes of *Genre = Drama AND Year > 1995*, the result was 2 watched movies and 0 unwatched movies. Which means probability(Watched = Yes) = 1; hence entropy = 1*log(1) = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculates the entropy of the given data set for the target attribute.\n",
    "def entropy(data, target_attr):\n",
    " \n",
    "    val_freq = {}\n",
    "    data_entropy = 0.0\n",
    " \n",
    "    # Calculate the frequency of each of the values in the target attr\n",
    "    for record in data:\n",
    "        if (val_freq.has_key(record[target_attr])):\n",
    "            val_freq[record[target_attr]] += 1.0\n",
    "        else:\n",
    "            val_freq[record[target_attr]]  = 1.0\n",
    " \n",
    "    # Calculate the entropy of the data for the target attribute\n",
    "    for freq in val_freq.values():\n",
    "        data_entropy += (-freq/len(data)) * math.log(freq/len(data), 2) \n",
    " \n",
    "    return data_entropy\n",
    "\n",
    "# Calculates the information gain (reduction in entropy) that would result by splitting the data on the chosen attribute (attr).\n",
    "def gain(data, attr, target_attr):\n",
    " \n",
    "    val_freq = {}\n",
    "    subset_entropy = 0.0\n",
    " \n",
    "    # Calculate the frequency of each of the values in the target attribute\n",
    "    for record in data:\n",
    "        if (val_freq.has_key(record[attr])):\n",
    "            val_freq[record[attr]] += 1.0\n",
    "        else:\n",
    "            val_freq[record[attr]]  = 1.0\n",
    " \n",
    "    # Calculate the sum of the entropy for each subset of records weighted by their probability of occuring in the training set.\n",
    "    for val in val_freq.keys():\n",
    "        val_prob = val_freq[val] / sum(val_freq.values())\n",
    "        data_subset = [record for record in data if record[attr] == val]\n",
    "        subset_entropy += val_prob * entropy(data_subset, target_attr)\n",
    " \n",
    "    # Subtract the entropy of the chosen attribute from the entropy of the whole data set with respect to the target attribute (and return it)\n",
    "    return (entropy(data, target_attr) - subset_entropy)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2010'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {'Title' : 'Inception', 'Year' : '2010'}\n",
    "target_attr = 'Year'\n",
    "data[target_attr]\n",
    "#entropy(data, target_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Year</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Length</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Watched?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>The Lord of the Rings: Return of the King</td>\n",
       "      <td>2003</td>\n",
       "      <td>Adventure, Drama, Fantasy</td>\n",
       "      <td>3h 21min</td>\n",
       "      <td>8.9</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Fight Club</td>\n",
       "      <td>1999</td>\n",
       "      <td>Drama</td>\n",
       "      <td>2h 19min</td>\n",
       "      <td>8.9</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>The Godfather</td>\n",
       "      <td>1972</td>\n",
       "      <td>Crime, Drama</td>\n",
       "      <td>2h 55min</td>\n",
       "      <td>9.2</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Title  Year  \\\n",
       "1 NaN  The Lord of the Rings: Return of the King  2003   \n",
       "2 NaN                                 Fight Club  1999   \n",
       "5 NaN                              The Godfather  1972   \n",
       "\n",
       "                       Genre    Length  Rating Watched?  \n",
       "1  Adventure, Drama, Fantasy  3h 21min     8.9        Y  \n",
       "2                      Drama  2h 19min     8.9        Y  \n",
       "5               Crime, Drama  2h 55min     9.2        N  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## import pandas as pd\n",
    "test = pd.read_csv('https://docs.google.com/spreadsheets/d/1fJ0DKxFkMr2XjzyspZFLv6yOlHJvl639Qpap0O4U78E/pub?gid=0&single=true&output=csv')\n",
    "test[test.Rating > 8.8]\n",
    "#print (test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "(MID.30) Without getting into the details of this equation, we can say that entropy can be numerically calculated. An entropy of 1 denotes the most randomness, while an entropy of 0 means least randomness. So this formula can help us determine how much an attribute decreases entropy of the segmentation it creates. This is also called *information gain* which measures the the change in entropy. Hence, we would want attributes that drive the highest information gain when we are trying to predict a target value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~streaming-demos/4.embed\" height=\"525\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.tools as tls\n",
    "tls.embed(\"https://plot.ly/~streaming-demos/4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Now lets take a look at the historical dataset. There are a total of 7 movies, with 3 of them watched and 4 unwatched movies. The probability of a watched movie is 3/7 = 42%. Lets partition this data into 2 segments based on the *Of Drama Genre Variable* into two segments. Segment 1 = *Of Drama Genre* = Y and Segment 2 = *Of Drama Genre* = N. Now Segment 1 has *Watched?* values of (N,Y, Y, N), which means a probability of Watched = 50% and Segment 2 has *Watched?* values of (N, Y, N) which means a probablity of Watched  = 33.3%. Comparing with the overall probability of of Watched - 42%, we have an important insight that movies of Genre = Drama have a higher probability of being watched. Now let us see if we can further segment this population by the *Released before 1995* variable. When we do this, we get (Y, Y). So we learn that a combination of "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Google](https://www.google.com)\n",
    "> ###Quote\n",
    "$x=y$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
